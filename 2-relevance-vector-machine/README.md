## Relevance Vector Machine

The SVM is a classic model that is characterised by characterised by the construction of an optimal separating hyperplane, defined by the "support vectors" of the dataset.
Furthermore, the kernel trick allows us to transform the input data into a higher-dimensional space, which enables the SVM to work in linearly unseparable data.

In "__Sparse Bayesian Learning and the Relevance Vector Machine__" (2001), Michael Tipping introduces the concept of Relevance Vector Machines, which adapt the SVM formulation into a fully Bayesian framework.

The RVM addresses three shortcomings of the SVM:
1. It reduces the computational complexity by eliminating the need of hyperparameter optimization (i.e. the slack variables in the SVM) and by only relying on low number of relevance vectors (analogous to support vectors)
2. Since it's a fully probabilistc model, it aims to estimate the complete posterior of the data instead of only providing a point estimate like the SVM
3. Lastly, unlike the SVM, RVMs can use kernels that don't satisfy Mercer's condition.

